# KAN-Transformer
A minimal implementation exploring Kolmogorov-Arnold Networks as alternatives for MLP feed-forward layers in Transformer-based language models. This project includes training and evaluation scripts for character-level language modeling on the Tiny Shakespeare dataset, with experiments on width, grid size, depth, and embedding dimension scaling.
